Name: Ian Delbridge
Email: idelbrid@u.rochester.edu
Course: CSC246
Homework: Homework 7, due Wed 3/30 5pm by turn_in script
Implement EM fitting of a mixture of gaussians on the two-dimensional data set points.dat. 
You should try different numbers of mixtures, as well as tied vs. separate covariance matrices for each gaussian. 
OR Implement EM fitting of the aspect model on the discrete data set pairs.dat. You should try different numbers of mixtures.

IN EITHER CASE Use the final 1/10 of the data for dev. Plot likelihood on train and dev vs iteration for different numbers of mixtures.

************ Files *********
clusters_4_random.png - plot of the clustering result on the training data with four gaussians and randomized initialization
gaussians.py - code for the mixture of gaussians model. Includes wrapper class and a script for this application
kmeans.py - wrapper class for kmeans clustering. Optionally used to initialize the gaussian mixture model
log_likelihood_test_4_gaussians.png - plot of log likelihood vs iterations of test data with 4 gaussians
log_likelihood_test_20gaussians.png - plot of log likelihood vs iterations of test data with 20 gaussians
log_likelihood_train_4_gaussians.png - plot of log likelihood vs iterations of train data with 4 gaussians
log_likelihood_train_20_gaussians.png - plot of log likelihood vs iterations of train data with 20 gaussians
README - this file


************ Algorithm *****
My algorithm's implementation is split into two phases: fit and predict. 
The fit function performs EM optimization to find the parameters of the gaussians. 
The predict function calculates the most likely gaussian for each point and the total likelihood of each point.

The fitting is performed by first initializing the parameters (mixing coefficients, means, and covariances) (in one of several ways to initialize). 
Then an estimation of the probability over the data is calculated using these parameters (mixing * normal(x ; mu, sigma)) / total

Then in the m-step, using the probabilty from the e-step, new parameters are set. They are determined by the maximum likelihood of the data. 
Until convergence or max iterations, we repeat the e-step, then the m-step.

The predict just calculates the likelihood of each point determined by the final parameters of the fitting algorithm, i.e. sum over gaussians ( mixing * normal(x ; mu, sigma) ).



************ Instructions ***
run the following command
python gaussians.py

Note that other configurations of the algorithm like alternate initialization schemes are hard-coded into the script that uses the mixture of gaussians class. 
However, even if kmeans initialization isn't used, kmeans.py should still be in the same directory as gaussians.py for the import. 

************ Results *******

[idelbrid@cycle1 MixtureOfGaussians]$ python gaussians.py
-2731.09358816 log likelihood on first 90% of data
-318.680738418 log likelihood on last 10% of data



************ Your interpretation *******
This application was easy to inspect visually to back up the math supporting the algorithm. So I made plots with the clustering output
for some of the results. It is difficult to evaluate how useful the likelihood is as a metric for the goodness of the result, because 
the relative likelihood never changed much from -320 to -330 on the test data, even when choosing ridiculously large numbers of gaussians.
Nonetheless, with the plots of the log likelihood versus number of iterations, it was evident that with too few iterations the model 
would not fit the data, but with too many iterations, it would over-fit; about 30 iterations seemed about right. When testing numbers of 
gaussians to use, I was surprised to find that there wasn't as much of a difference in log-likelihod between using 3 or 4 gaussians and
using way too many, like 20. Visually inspecting the data, it definitely looks like there ought to be 3 or 4 clusters though their
distributions seem to uniform in an area to be gaussian. 

Additionally, these clusters were very close together and overlapping, which
made the problem much harder than completely disjoint clusters. 

Finally, the initialization schemes didn't make that much of a difference.
If enough iterations were made, they all converged to about the same thing (for a given number of gaussians). However, in fact, the 
book suggested using k-means to initialize the means and covariances, but I found that in general this method of initialization caused 
an increase in the log-likelihood. 

************ References ************
Textbook, class notes, NB notes...

